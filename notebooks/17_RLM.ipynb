{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f77143b4-62a7-47d5-ba7f-a508a9f1eee5",
   "metadata": {},
   "source": [
    "# Análisis de regresión\n",
    "\"*Todos los modelos están equivocados, pero algunos son útiles\" G.E.P. Box (1919-2013)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c5b02f-47a1-41d7-9988-e59525ccdaad",
   "metadata": {},
   "source": [
    "*¿Para qué se utiliza?*\n",
    "El análisis de regresión lineal es una técnica estadística utilizada para modelar la relación entre una variable dependiente y una o más variables independientes. Su propósito principal es explicar o predecir el comportamiento de la variable dependiente a partir de variables explicativas.\n",
    "\n",
    "Algunos ejemplos de uso son:\n",
    "- Pronosticar las ventas semanales a partir del gasto en Facebook Ads y Google Ads. Este modelo ayudaría a identificar qué canal tiene mayor impacto y cómo distribuir mejor el presupuesto.\n",
    "- Explicar cómo varía el precio de una acción en función del tipo de cambio y la tasa de interés nacional. Este modelo ayudaría a tomar decisiones de inversión o de cobertura ante cambios macroeconómicos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ed06c-83e2-4c9a-bb0b-a4226070444a",
   "metadata": {},
   "source": [
    "*Forma general del modelo*\n",
    "\n",
    "Se asume que esta relación sigue una ecuación lineal conocida como ecuación de regresión. En su forma básica la ecuación o modelo de regresión múltiple se expresa como:\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 +...+ \\beta_k x_k + \\varepsilon\n",
    "$$\n",
    "donde:\n",
    "- $y$ es la variable dependiente, aquella que se desea explicar o predecir\n",
    "- $x_1, x_2,...,x_n$ son las variable independientes, que sirven para explicar $y$. Si solo hay una variable independiente se conoce como *regresión lineal simple*, si hay dos o más, se trata de una *regresión lineal múltiple*.\n",
    "- $\\beta_0$ es el término de intersección (también llamado constante o intercepto), que representa el valor de $y$ si todas las variables independientes fueran igual a cero. Este valor solo tiene interpretación válida en algunos contextos.\n",
    "- $\\beta_1, \\beta_2,..., \\beta_k$ son los coeficientes de regresión, que indican el cambio estimado en la variable dependiente por unidad de cambio en la respectiva variable independiente, manteniendo todo lo demás constante.\n",
    "- $\\varepsilon$ es el término de error, que captura la variabilidad en $y$ que no es explicada por las variables independientes. Cuando se analiza un modelo ajustado a datos específicos, es más preciso referirse a este error como *residual*.\n",
    "\n",
    "*Variables consideradas*\n",
    "- La variable dependiente debe ser cuantitativa continua (medida en escala de intervalo o razón).\n",
    "- Las variables independientes pueden ser cuantitativas o categóricas (estas últimas se codifican mediante variables ficticias o \"dummies\").\n",
    "- Para que el modelo sea útil, al menos una variable independiente debe ser cuantitativa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284b2b7c-065d-4894-9e9f-765fc0ec00f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "*Supuestos básicos del modelo*\n",
    "\n",
    "Las estimaciones de los coeficientes de regresión serán óptimas según el teorema de Gauss-Markov, siempre que el término de error cumpla con los siguientes supuestos:\n",
    "- El error tiene media igual a cero. Esto implica que, en promedio, los errores no sesgan las predicciones del modelo.\n",
    "- Los errores son independientes. No existe correlación entre los errores de diferentes observaciones.\n",
    "- Homocedasticidad: La varianza del error es constante para todos los valores de las variables independientes.\n",
    "- Distribución normal del error (en caso de inferencia estadística). Este supuesto es requerido para realizar pruebas de hipótesis y construir intervalos de confianza.\n",
    "\n",
    "Estas condiciones se pueden resumir en la notación $\\varepsilon$~N<sub>iid</sub> (0,σ<sup>2</sup>). Si se cumplen estas condiciones, los estimadores de los coeficientes de regresión serán los mejores estimadores insesgados de mínima varianza (*MELI* por las siglas en español; *BLUE* por las siglas en inglés de *Best Linear Unbiased Estimators*). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2da95a0-b300-479d-b31d-35af2b1462b8",
   "metadata": {},
   "source": [
    "*Recomendaciones sobre el tamaño de la muestra*\n",
    "\n",
    "Existen diversas recomendaciones para determinar el tamaño de muestra adecuado en un análisis de regresión. Para generalizar los resultados, se sugiere contar con al menos 5 observaciones por cada variable independiente, aunque lo ideal es tener entre 15 y 20 observaciones por variable independiente (Hair, Black, Babin, & Anderson, 2009).\n",
    "\n",
    "Por otro lado, Green (1991) argumenta que no es adecuado establecer un tamaño de muestra fijo (por ejemplo, 100 observaciones). En su lugar, propone las siguientes reglas:\n",
    "- Para evaluar un modelo de regresión, se recomienda un mínimo de 50 + 8k observaciones, donde k es el número de variables independientes. \n",
    "- Para evaluar los coeficientes individuales de cada variable independiente, se sugiere un mínimo de 104 + k observaciones.\n",
    "Más allá del tamaño de la muestra, es fundamental asegurarse de que esta sea representativa de la población de interés para garantizar la validez de los resultados.\n",
    "\n",
    "*Consideraciones sobre las variables independientes*\n",
    "- Se debe evitar la multicolinealidad, es decir, que dos o más variables independientes estén altamente correlacionadas, ya que esto puede distorsionar la estimación de los coeficientes.\n",
    "- Un criterio práctico inicial es revisar la matriz de correlaciones entre variables independientes y evitar aquellas con r > 0.90.\n",
    "- De forma más formal, se puede utilizar el Factor de Inflación de la Varianza (VIF) para detectar problemas de multicolinealidad.\n",
    "\n",
    "*¿Qué permite hacer la regresión lineal?*\n",
    "- Evaluar el efecto de cada variable independiente sobre la variable dependiente (significancia estadística).\n",
    "- Medir la magnitud y dirección del efecto mediante los coeficientes estimados.\n",
    "- Predecir valores futuros de la variable dependiente dados ciertos valores de las independientes.\n",
    "- Medir el ajuste del modelo con estadísticas como el coeficiente de determinación (R cuadrada), que indica qué proporción de la variabilidad de Y es explicada por el modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd5df22-995a-41f0-a2d4-4047a2ca88ac",
   "metadata": {},
   "source": [
    "## Análisis de regresión\n",
    "Utilizaremos un modelo de regresión para estimar el precio de una casa en una zona de la ciudad. Como primer paso importamos las bibliotecas a utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be85504c-8bff-4b4e-a2c9-6dbaa12ac32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas\n",
    "#import warnings\n",
    "#warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6181f8-5fe3-46ea-a1ca-ba844003f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el archivo \"data/casas.xlsx\"\n",
    "df = pd.read_excel('https://github.com/adan-rs/AnalisisDatos/raw/main/data/casas.xlsx')\n",
    "# Para este ejemplo seleccionamos casas (tipo = 0 )\n",
    "df = df[df['tipo']==0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345d191c-fb3d-4d67-b8ef-4f13a64a2f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisar las variables y el número de observaciones\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88755fec-cf78-476d-89e7-50a0259c3458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisar las primeras filas del DataFrame\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55fa5d3-b327-4637-89c5-528e94ff5585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifica las variables cuantitativasy obtener la estadística descriptiva\n",
    "var_cont = df[['preciomillones', 'recamaras', 'baños', 'construccion']]\n",
    "var_cont.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf20ab8b-269a-4238-b426-03b5ca2a514f",
   "metadata": {},
   "source": [
    "Un aspecto clave en la selección de las variables independientes es evitar la multicolinealidad, que ocurre cuando una combinación lineal de dos o más variables contiene la misma información que otra variable independiente. Aunque existen técnicas formales para detectar la multicolinealidad (como el factor de inflación de la varianza, VIF), un primer criterio práctico es evitar correlaciones altas entre variables independientes, por ejemplo, valores de $r>0.90$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27021f61-197f-4cca-a758-8528fd5cac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos la matriz de correlaciones\n",
    "var_cont.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3083a27c-8224-40f3-9446-a73678dbcf89",
   "metadata": {},
   "source": [
    "La función `sns.pairplot` de *Seaborn* es útil para visualizar las relaciones entre pares de variables en un DataFrame. Sus principales parámetros son:\n",
    "- `data`: Especifica el DataFrame con las variables a graficar.\n",
    "- `corner`: Si se establece en True, se muestra solo la matriz triangular inferior, lo que reduce la redundancia en la visualización.\n",
    "- `kind`: Define el tipo de gráfico. Por defecto, usa \"scatter\", pero también puede configurarse como \"reg\" para incluir una recta de regresión en cada gráfico de dispersión.\n",
    "- `markers`: Permite definir el tipo de marcador en el gráfico. Por ejemplo, usar '+' en lugar de '.' cambia el estilo de los puntos.\n",
    "- `height`: Controla la altura (en pulgadas) de cada gráfico dentro de la matriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557c9378-a199-4d7c-8d28-a5d4c3cf55db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos las relaciones entre variables\n",
    "sns.pairplot(var_cont, corner=True, kind='reg', markers='+', height=1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75ac202-64b9-40d7-9bbb-7aa54aa5b0da",
   "metadata": {},
   "source": [
    "### Estimación del modelo (con StatsModels)\n",
    "Utilizaremos primero la biblioteca *StatsModels* para realizar el análisis de regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b7c6c2-2234-451e-9e8d-efc31b3c0e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar la librería\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2171e4b3-65dd-40a3-98a8-75fdb3be0ecb",
   "metadata": {},
   "source": [
    "Es recomendable crear un DataFrame `X` que contenga las variables independientes y otro `y` con la variable dependiente.\n",
    "\n",
    "Para seleccionar las variables independientes, primero es importante evaluar cuáles pueden ser relevantes para el modelo. Si hay muchas variables independientes, se puede aplicar un *Análisis Factorial* u otras técnicas de reducción de dimensionalidad, como *Análisis de Componentes Principales (PCA)*.\n",
    "\n",
    "En este ejemplo, comenzaremos con la variable \"construcción\" como predictor. Para utilizar la función de StatsModels, es necesario agregar manualmente una columna de unos (`1`) al DataFrame X, lo que representa la constante en el modelo de regresión. Esto se hace con `sm.add_constant()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f07f10-2b2b-4069-a1d6-a0a62e151ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un DataFrame con las v. indep. y la v. dependiente\n",
    "X = df[['construccion', 'baños']]\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "y = df['preciomillones']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d31f60e-c8a4-414b-9dcb-7001b1d801a9",
   "metadata": {},
   "source": [
    "Corremos el modelo de regresión mediante mínimos cuadrados ordinarios:  \n",
    "`regresion = sm.OLS(y, X).fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d766fbb-b19f-41ae-bb67-f98c8fd975af",
   "metadata": {},
   "outputs": [],
   "source": [
    "regresion = sm.OLS(y, X).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa841e6-bfc6-434f-af72-431c14e4d4ed",
   "metadata": {},
   "source": [
    "El resultado se muestra con la instrucción\n",
    "`summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7620f5f5-a2bf-42f6-94f1-4f5eea1dc6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "regresion.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb38446-06fc-4400-af27-f457071b6063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar coeficientes de regresión\n",
    "regresion.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50bbe5f-746b-4264-955c-c6d78cce888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar residuales\n",
    "residuales = regresion.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debdee7e-eac0-4145-bed1-2cb6586f5e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener valores estimados\n",
    "y_hat = regresion.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9917ed3-dfd9-4c65-900f-ecdf36f49d07",
   "metadata": {},
   "source": [
    "Ejemplo de pronóstico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f8d571-d4e9-46bb-bd01-2773d0945cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pronóstico\n",
    "nuevos_valores = [1,500,2]\n",
    "regresion.predict(nuevos_valores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597425bf-d41a-4cbc-bace-e0378491bed0",
   "metadata": {},
   "source": [
    "## Interpretación de los resultados\n",
    "Al ajustar un modelo de regresión con StatsModels, se generan diversas métricas que permiten evaluar su calidad y ajuste. A continuación, se explican algunas de las más relevantes:\n",
    "- *R-squared (R²)*: Indica el porcentaje de la variabilidad de la variable dependiente que es explicada por el modelo. Valores más cercanos a 1 son mejores.\n",
    "- *Adj. R-squared (R² ajustado)*: Penaliza la inclusión de variables no significativas, por lo que se usa para comparar modelos con diferente número de variables independientes.\n",
    "- *AIC (Criterio de Información de Akaike) / BIC (Criterio de Información Bayesiano)*: Son medidas relativas para comparar modelos de regresión múltiple. Valores menores indican modelos más eficientes en términos de ajuste y simplicidad.\n",
    "- *F-statistic*: Evalúa la significancia global del modelo. Su hipótesis nula es que todos los coeficientes de regresión son cero (es decir, que el modelo no explica la variable dependiente). Un p-valor bajo sugiere que al menos una variable independiente es significativa.\n",
    "- *Skew (Sesgo)*: Mide la asimetría de la distribución de los residuales. Valores cercanos a 0 indican una distribución simétrica.\n",
    "- *Kurtosis*: Indica qué tan puntiaguda o achatada es la distribución de los residuales en comparación con una normal.\n",
    "- *Prob(Omnibus) y Jarque-Bera (JB)*: Son pruebas para evaluar la normalidad de los residuales. Un p-valor alto sugiere que los residuales siguen una distribución normal, lo que es deseable.\n",
    "- *Durbin-Watson*: Detecta autocorrelación en los errores. Valores cercanos a 2 son deseables. Valores entre 0 y 2 indican autocorrelación positiva. Valores entre 2 y 4 indican autocorrelación negativa.\n",
    "- *Cond. No. (Número de condición)*: Evalúa posibles problemas de multicolinealidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f33e7c-05e0-4b53-87e4-ce11867ef6d0",
   "metadata": {},
   "source": [
    "### Ajuste del modelo\n",
    "La métrica más utilizada para medir el ajuste del modelo es el *coeficiente de determinación* (R<sup>2</sup>) que mide la proporción de la variación de la variable dependiente que es explicada por el modelo. \n",
    "\n",
    "En un modelo de regresión lineal, la variación total de los datos se mide con sumas de cuadrados:\n",
    "$$\n",
    "SS_{total} = SS_{modelo} + SS_{residual}\n",
    "$$\n",
    "Donde:\n",
    "\n",
    "$SS_{total} = \\sum_{i = 1}^{n} (y-\\bar{y})^2$ es la suma de cuadrados total  \n",
    "$SS_{modelo} = \\sum_{i = 1}^{n} (\\hat{y}-\\bar{y})^2$ es la suma de cuadrados del modelo también conocida como suma de cuadrados explicada  \n",
    "$SS_{residual} = \\sum_{i = 1}^{n} (y-\\hat{y})^2$ es la suma de cuadrados del residual o suma de cuadrados del error\n",
    "\n",
    "El coeficiente de determinación es equivalente a\n",
    "\n",
    "$$\n",
    "R^2 = SS_{modelo} / SS_{total}\n",
    "$$\n",
    "\n",
    "La R cuadrada toma valores entre 0 y 1, donde valores más grandes indican un mejor ajuste. Sin embargo, no existen criterios únicos de qué valor de R cuadrada es aceptable, sino que ello depende de las características del estudio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d1ce90-c60e-40c6-8f2d-2f647e011d6b",
   "metadata": {},
   "source": [
    "## Evaluación de supuestos\n",
    "### Homocedasticidad\n",
    "Uno de los supuestos del modelo de regresión es que la varianza de los residuales se mantiene constante (homocedasticidad). El cumplimiento de este supuesto se puede explorar en la gráfica del residual estandarizado versus los valores pronosticados. \n",
    "- Un patrón aleatorio en la dispersión de los residuales sugiere homocedasticidad.\n",
    "- Un patrón de dispersión en forma de abanico (creciente o decreciente) indicaría una posible violación a este supuesto, lo que sugiere heterocedasticidad.\n",
    "\n",
    "Existen varias pruebas para evaluar la presencia de heterocedasticidad:\n",
    "- *Breusch-Pagan*: Evalúa la relación entre los residuales al cuadrado y las variables independientes. Un resultado significativo indica heterocedasticidad..\n",
    "- *Prueba de White*: Similar a la Breush-Pagan, pero más general, ya que considera términos no lineales y la correlación serial.\n",
    "- *Prueba de Goldfeld-Quandt*: Divide los datos en dos grupos según los valores de una variable independiente y compara las varianzas de los residuales en ambos grupos. Una diferencia significativa sugiere heterocedasticidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf773e65-15f8-4d70-92ae-19aec0b30b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de residuales\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.scatter(y_hat, residuales, marker='.', color='b')\n",
    "plt.xlabel('y hat')\n",
    "plt.ylabel('residuales');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd3b162-4c0a-45fa-bb08-6f8d3f3dc4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "# Asumiendo que ya se tiene el modelo y los residuales\n",
    "lm, lm_p_value, fvalue, f_p_value = het_breuschpagan(residuales, X)\n",
    "\n",
    "print(\"Estadístico LM:\", lm)\n",
    "print(\"Valor p del estadístico LM:\", lm_p_value)\n",
    "print(\"Estadístico F:\", fvalue)\n",
    "print(\"Valor p del estadístico F:\", f_p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc183c02-9ff9-4920-9388-068e3e5a3371",
   "metadata": {},
   "source": [
    "Si el p-valor es menor que 0.05 es evidencia de la presencia de heteroscedasticidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b17e6cb-4ec9-437b-a636-ba6929f23ed0",
   "metadata": {},
   "source": [
    "*¿Qué hacer si se detecta heteroscedasticidad?*\n",
    "- Explorar transformaciones de variables (p. ej. logaritmos)\n",
    "- Utilizar otros modelos de regresión, como Mínimos Cuadrados Ponderados (WLS) que asigna pesos inversamente proporcionales a la varianza de los errores\n",
    "- Crear modelos separados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7919bfaf-c92e-4e3e-a8ca-7061b0f00ed0",
   "metadata": {},
   "source": [
    "### Normalidad\n",
    "La normalidad en la distribución de los errores y su media se puede verificar en el histograma de los residuales y una prueba de normalidad (como Shapiro-Wilks o  Kolmogorov-Smirnov). En la prueba de normalidad lo deseable es obtener un p-valor mayor a 0.05 (no se rechaza la hipótesis nula de normalidad en los datos). \n",
    "\n",
    "El reporte de salida de StatsModelo muestra el p-valor de las pruebas omnibus y de Jarque-Bera, ambas basadas en el sesgo y la curtosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df049f4-1857-4f6c-82ff-5d21a3cac07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3)) \n",
    "sns.histplot(residuales, color='b');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f75edc-76ec-472f-b6b4-86826f430d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "stat, p_value = shapiro(residuales)\n",
    "print(\"Estadístico de prueba:\", stat)\n",
    "print(\"Valor p:\", p_value)\n",
    "\n",
    "# Interpretar los resultados\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Se rechaza la hipótesis nula de normalidad en los datos\")\n",
    "else:\n",
    "    print(\"No se puede rechazar la hipótesis nula de normalidad en los datos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ee2c61-b724-45c9-89a2-021dfb19b57c",
   "metadata": {},
   "source": [
    "*¿Qué hacer si se detecta no normalidad en los residuales?*\n",
    "\n",
    "- Revisar outliers o valores influyentes.\n",
    "- Explorar transformaciones de variables (p. ej. logaritmos)\n",
    "- Utilizar otros modelos de regresión.\n",
    "- Explorar variables no incluidas en el modelo de regresión.\n",
    "- Utilizar bootstrapping para las estimaciones de intervalos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ef4d76-7407-46c1-8ce1-58ee45bd9af5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Independencia en los errores\n",
    "Los errores deben ser independientes en una regresión, esto es, no relacionados entre sí.  Generalmente esto implica evaluar la autocorrelación (correlación entre valores adyacentes) y se lleva a cabo mediante la prueba de Durbin-Watson (solamente en series de tiempo). En esta prueba, el estadístico de prueba puede tomar valores entre 0 y 4. Si el estadístico es cercano a 2 entonces los residuales no están correlacionados. Valores menores a 1 o mayores a 3 son señal de un problema serio de autocorrelación. \n",
    "\n",
    "*¿Qué hacer si se detecta autocorrelación en los residuales?*\n",
    "- Explorar modelos específicos para series temporales (ARIMA, SARIMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acf3d4c-a8b9-423a-a3f5-765d8525601b",
   "metadata": {},
   "source": [
    "### Multicolinealidad\n",
    "La multicolinealidad se presenta cuando existe una fuerte correlación entre dos o más variables independientes o una combinación lineal de ellas. La multicolinealidad puede llevar a tener un modelo estadísticamente significativo, pero coeficientes de regresión no significativos.\n",
    "\n",
    "Para evaluar la multicolinealidad:\n",
    "- Revisar la matriz de correlaciones de las variables independientes para identificar correlaciones arriba de 0.90\n",
    "- Revisar el número de condición, valores mayores a 30 indicarían un problema grave\n",
    "- Calcular el Factor de Inflación de Varianza de cada variable, valores mayores a 10 indicarían un problema grave\n",
    "\n",
    "*¿Qué hacer si se detecta multicolinealidad?*\n",
    "- Regularizar (regresión Lasso o Ridge).\n",
    "- Combinar variables.\n",
    "- Eliminar variables redundantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c24a73-beda-4e7e-a925-b9c4a45242b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "X = df[['construccion', 'baños']]\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Calcular valores VIF para cada variable\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Variable'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "vif_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc2c9a0-a9ef-42b2-aee3-b05f399a5ea7",
   "metadata": {},
   "source": [
    "## Extra: uso de scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b489dc-b5de-4afc-9a06-49229a17d71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar la biblioteca\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23965f7a-fe53-453f-9976-26fef1d80a95",
   "metadata": {},
   "source": [
    "Con *SciKit-Learn' no se agrega la columna de constantes como en StatsModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcbb2df-316e-4cb1-a61a-ee132f86762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['construccion']]\n",
    "y = df['preciomillones']\n",
    "model = LinearRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d1a36-568b-4a7c-b8bc-8ae37e76483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los coeficientes de regresión\n",
    "model.intercept_, model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee91ab2-166b-48cd-8a33-c58bad126867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener y estimada (y hat)\n",
    "y_hat = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f93b457-c105-43ee-bf1e-b53597cbea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener residuales\n",
    "residuales = (y - y_hat).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b15581-138d-447d-83ff-77a2ac1431be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular R cuadrada\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49669161-4dbb-4cd8-9683-343e76c164f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}